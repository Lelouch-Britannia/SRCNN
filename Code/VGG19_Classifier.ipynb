{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 VGG 19 Classifier(SR and HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import wandb\n",
    "    import yaml\n",
    "    import torchinfo\n",
    "except:\n",
    "    %pip install wandb\n",
    "    %pip install pyyaml\n",
    "    %pip install torchinfo\n",
    "    import yaml\n",
    "    import wandb\n",
    "    import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1(a) VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "class VGG19Classifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(VGG19Classifier, self).__init__()\n",
    "\n",
    "        # Adjusting to use the new weights parameter\n",
    "        vgg19 = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        features_layers = 24\n",
    "        self.features = nn.Sequential(*list(vgg19.features.children())[:features_layers+1]) \n",
    "\n",
    "        # Freeze the features layers\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=64, out_features=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the features\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # Load the weights from the pre-trained model present locally\n",
    "    def load_weights(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key\n",
    "os.environ['WANDB_API_KEY'] = '0736c590933a18ad9639f49867ed1548495ded1c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wandb entity and project\n",
    "project_name = \"SRCNN+VGG\"  # replace with your wandb project name\n",
    "entity_name = \"pershadmayank\"  # replace with your wandb entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "classifier = VGG19Classifier(num_features=28*28*512).to(device)\n",
    "summary(classifier, input_size=[1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 `train_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: str,\n",
    "               max_pixel_value: float = 1.0) -> Tuple[float, float]:\n",
    "  \"\"\"\n",
    "  Performs a single training step including forward pass, loss computation,\n",
    "  backpropagation, and optimizer step.\n",
    "\n",
    "  Parameters:\n",
    "  - model (torch.nn.Module): The neural network model to be trained.\n",
    "  - dataloader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "  - loss_fn: Loss function used for training.\n",
    "  - optimizer (torch.optim.Optimizer): Optimizer used for parameter updates.\n",
    "  - device (str): Device to run the training on ('cuda' or 'cpu').\n",
    "  - max_pixel_value (float, optional): The maximum pixel value used in the PSNR calculation. Default is 1.0.\n",
    "\n",
    "  Returns:\n",
    "  - train_loss (float): Average training loss for this step.\n",
    "  - train_psnr (float): Average Peak Signal-to-Noise Ratio (PSNR) for this step.\n",
    "\n",
    "  Raises:\n",
    "  - ValueError: If `device` is not 'cuda' or 'cpu'.\n",
    "  - TypeError: If the provided model, dataloader, loss function, or optimizer are of the wrong type.\n",
    "  \"\"\"\n",
    "\n",
    "  # Validate input parameters for type and value\n",
    "  if not isinstance(model, torch.nn.Module):\n",
    "    raise TypeError(\"model must be an instance of torch.nn.Module\")\n",
    "  if not isinstance(dataloader, torch.utils.data.DataLoader):\n",
    "    raise TypeError(\"dataloader must be an instance of torch.utils.data.DataLoader\")\n",
    "  if not callable(loss_fn):\n",
    "    raise TypeError(\"loss_fn must be callable\")\n",
    "  if not isinstance(optimizer, torch.optim.Optimizer):\n",
    "    raise TypeError(\"optimizer must be an instance of torch.optim.Optimizer\")\n",
    "  if device not in ['cuda', 'cpu']:\n",
    "    raise ValueError(\"device must be 'cuda' or 'cpu'\")\n",
    "\n",
    "  # Ensure model is on the correct device\n",
    "  model.to(device)\n",
    "\n",
    "  # Put the model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and PSNR\n",
    "  train_loss = 0.0\n",
    "  train_acc = 0.0\n",
    "\n",
    "  # Loop through batches of data\n",
    "  for _, (X, y) in enumerate(dataloader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate and accumulate metrics across all the batches\n",
    "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "    train_acc = (y_pred_class == y).sum().item() / len(y_pred)\n",
    "    \n",
    "  # Adjust metrics to get average loss and PSNR per batch\n",
    "  train_loss /= len(dataloader)\n",
    "  train_acc /= len(dataloader)\n",
    "\n",
    "  return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 `test_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn,\n",
    "              device: str,\n",
    "              max_pixel_value: float = 1.0) -> Tuple[float, float]:\n",
    "  \"\"\"\n",
    "  Performs a single evaluation step, calculating the average loss and PSNR\n",
    "  over the provided dataloader.\n",
    "\n",
    "  Parameters:\n",
    "  - model (torch.nn.Module): The neural network model to be evaluated.\n",
    "  - dataloader (torch.utils.data.DataLoader): DataLoader for the dataset to evaluate.\n",
    "  - loss_fn: The loss function used for evaluation.\n",
    "  - device (str): The device to run the evaluation on ('cuda' or 'cpu').\n",
    "  - max_pixel_value (float, optional): The maximum pixel value used in the PSNR calculation. Default is 1.0.\n",
    "\n",
    "  Returns:\n",
    "  - test_loss (float): The average loss over the dataloader.\n",
    "  - test_psnr (float): The average Peak Signal-to-Noise Ratio over the dataloader.\n",
    "\n",
    "  Raises:\n",
    "  - ValueError: If `device` is not 'cuda' or 'cpu'.\n",
    "  - TypeError: If the provided model, dataloader, loss function, or device are of the wrong type.\n",
    "  \"\"\"\n",
    "\n",
    "  # Validate input parameters\n",
    "  if not isinstance(model, torch.nn.Module):\n",
    "    raise TypeError(\"model must be an instance of torch.nn.Module\")\n",
    "  if not isinstance(dataloader, torch.utils.data.DataLoader):\n",
    "    raise TypeError(\"dataloader must be an instance of torch.utils.data.DataLoader\")\n",
    "  if not callable(loss_fn):\n",
    "    raise TypeError(\"loss_fn must be callable\")\n",
    "  if device not in ['cuda', 'cpu']:\n",
    "    raise ValueError(\"device must be 'cuda' or 'cpu'\")\n",
    "\n",
    "  # Ensure model is on the correct device\n",
    "  model.to(device)\n",
    "\n",
    "  # Put the model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  test_loss = 0.0\n",
    "  test_psnr = 0.0\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    for _, (X, y) in enumerate(dataloader):\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      test_pred = model(X)\n",
    "\n",
    "      # Calculate loss\n",
    "      loss = loss_fn(test_pred, y)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      # Calculate and accumulate acc\n",
    "      test_pred_labels = torch.argmax(torch.softmax(test_pred, dim=1), dim=1)\n",
    "      test_acc = (test_pred_labels == y).sum().item() / len(test_pred_labels)\n",
    "\n",
    "  # Compute average loss and PSNR\n",
    "  test_loss /= len(dataloader)\n",
    "  test_acc /= len(dataloader)\n",
    "\n",
    "  return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Checkpoint Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, path=\"checkpoint.pth\"):\n",
    "  \"\"\"\n",
    "  Saves a checkpoint of the model and optimizer state.\n",
    "\n",
    "  Parameters:\n",
    "  - epoch: The current epoch number.\n",
    "  - model: The model being trained.\n",
    "  - optimizer: The optimizer being used for training.\n",
    "  - loss: The loss value at the checkpoint.\n",
    "  - path: The path to save the checkpoint to.\n",
    "  \"\"\"\n",
    "  checkpoint = {\n",
    "    'epoch': epoch + 1,  # Saving such that training can resume from the next epoch\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "  }\n",
    "  torch.save(checkpoint, path)\n",
    "  print(f\"Checkpoint saved at '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_checkpoints(directory):\n",
    "    \"\"\"\n",
    "    Deletes all files within the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory: The path to the directory whose files are to be deleted.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(directory):\n",
    "        # Iterate through all files in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                # If it is a file, delete it\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                # If it is a directory, delete it and all its contents\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        print(f\"All checkpoints in '{directory}' have been deleted.\")\n",
    "    else:\n",
    "        print(f\"The directory {directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Early Stopping and Saving Best Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "  def __init__(self, patience=7, verbose=False, delta=0, path='best_model.pth'):\n",
    "    self.patience = patience\n",
    "    self.verbose = verbose\n",
    "    self.delta = delta\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "    self.val_loss_min = np.Inf\n",
    "    self.counter = 0\n",
    "    self.path = path\n",
    "\n",
    "  def __call__(self, val_loss, model, optimizer):\n",
    "    score = -val_loss\n",
    "\n",
    "    if self.best_score is None:\n",
    "      self.best_score = score\n",
    "      self.save_checkpoint(val_loss, model, optimizer)\n",
    "    elif score < self.best_score + self.delta:\n",
    "      self.counter += 1\n",
    "      if self.verbose:\n",
    "          print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "      if self.counter >= self.patience:\n",
    "          self.early_stop = True\n",
    "    else:\n",
    "      self.best_score = score\n",
    "      self.save_checkpoint(val_loss, model, optimizer)\n",
    "      self.counter = 0\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model, optimizer):\n",
    "    '''Saves model when validation loss decrease.'''\n",
    "    if self.verbose:\n",
    "      print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': val_loss,\n",
    "    }, self.path)\n",
    "    self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Combining `train_step()` and `test_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import signal\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn,\n",
    "          path: str,\n",
    "          start_epoch: int = 0,\n",
    "          end_epoch: int = 200,\n",
    "          checkpoint_interval=20,\n",
    "          device: str = device):\n",
    "\n",
    "  # 1.0 Start the timer\n",
    "  start_time = time.time()\n",
    "\n",
    "  # 2.0 Create a empty result dicitonary\n",
    "  results = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "  # 3.0 Directory to save the model\n",
    "  model_dir = \"hard-disk-2/users/mpershad/SRCNN_DIV2K/Models/Classifier\"\n",
    "  if not (os.path.exists(model_dir)):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "  model_path = f\"{model_dir}/{path}.pth\"\n",
    "\n",
    "  # 4.0 Calculate Logging Interval(10% of total epochs)\n",
    "  total_epochs = end_epoch - start_epoch\n",
    "  log_interval = total_epochs // 10 if total_epochs // 10 > 0 else 1\n",
    "  \n",
    "  # 5.0 Initialize the early stopping\n",
    "  early_stopper = EarlyStopping(patience=int(total_epochs/5), verbose=True, delta=5e-6, path=model_path)\n",
    "\n",
    "  # --- Signal handling for KeyboardInterrupt ---\n",
    "  def signal_handler(signal, frame):\n",
    "    print(\"Training stopped by user! Saving a checkpoint before exiting...\")\n",
    "    checkpoint_path = f\"{model_dir}/INTERRUPTED_{path}.pth\"\n",
    "    save_checkpoint(epoch=epoch, model=model, optimizer=optimizer, loss=val_loss, path=checkpoint_path)\n",
    "    print(f'Checkpoint saved. Safetly terminated training.')\n",
    "\n",
    "  # Register the signal handler\n",
    "  signal.signal(signal.SIGINT, signal_handler)\n",
    "  \n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(start_epoch, end_epoch)):\n",
    "\n",
    "    # 6.0 Training step\n",
    "    train_loss, train_acc = train_step(model=model,\n",
    "                            dataloader=train_dataloader,\n",
    "                            loss_fn=loss_fn,\n",
    "                            optimizer=optimizer,\n",
    "                            device=device)\n",
    "\n",
    "    # 7.0 Testing step(validation)\n",
    "    val_loss, val_acc = test_step(model=model,\n",
    "                          dataloader=test_dataloader,\n",
    "                          loss_fn=loss_fn,\n",
    "                          device=device)\n",
    "\n",
    "    # 8.0 Update results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"val_loss\"].append(val_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    # Log values to wandb\n",
    "    if wandb.run:\n",
    "      wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc})\n",
    "\n",
    "    # Log values every 10% of the total epochs\n",
    "    if epoch % log_interval == 0 or epoch == end_epoch - 1:\n",
    "      \n",
    "      # Print out what's happening\n",
    "      print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # 9.0 Early stopping\n",
    "    early_stopper(val_loss, model, optimizer)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    checkpoint_dir = f\"/hard-disk-2/users/mpershad/SRCNN_DIV2K/Checkpoints/Classifier/{path}\"\n",
    "\n",
    "    # 10.0 Checkpoint Saving\n",
    "    if epoch % checkpoint_interval == 0 or epoch == end_epoch - 1:\n",
    "\n",
    "      # Create the directory for saving Checkpoints\n",
    "      if not (os.path.exists(checkpoint_dir)):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "      checkpoint_path = f\"{checkpoint_dir}/epoch_{epoch}.pth\"\n",
    "\n",
    "      save_checkpoint(epoch=epoch, model=model, optimizer=optimizer, loss=val_loss, path=checkpoint_path)\n",
    "      \n",
    "      # Optionally log checkpoint to wandb if connected\n",
    "      if wandb.run:\n",
    "        wandb.save(checkpoint_path, base_path=\"/hard-disk-2/users/mpershad/SRCNN_DIV2K/Checkpoints/Classifier\")\n",
    "\n",
    "  # 11.0 Use wandb.save() to ensure the model file is saved to W&B if connected\n",
    "  if wandb.run:\n",
    "    wandb.save(model_path, base_path=model_dir)\n",
    "\n",
    "    # 12.0 Log the model as an artifact if connected\n",
    "    artifact = wandb.Artifact('Classifier', type='model', description=\"A super-resolution model\")\n",
    "    artifact.add_file(model_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "  # 13.0 Calculate and log training duration\n",
    "  end_time = time.time()\n",
    "  total_training_time = end_time - start_time\n",
    "  if wandb.run:\n",
    "    wandb.log({'total_training_time': total_training_time})\n",
    "\n",
    "  print(f\"Total training time: {total_training_time:.3f} seconds\")\n",
    "\n",
    "  wandb.finish()\n",
    "\n",
    "  delete_checkpoints(checkpoint_dir)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(results):\n",
    "    \"\"\"Plot training and validation loss curves and PSNR curves if available.\"\"\"\n",
    "    # Setup the figure and axes dynamically based on what needs to be plotted\n",
    "    plots_needed = sum(key in results for key in ['train_loss', 'val_loss', 'train_psnr', 'val_psnr'])\n",
    "    if plots_needed == 0:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "    fig, axes = plt.subplots(1, plots_needed, figsize=(6 * plots_needed, 6))\n",
    "    if plots_needed == 1:\n",
    "        axes = [axes]  # Make it iterable\n",
    "\n",
    "    current_plot = 0\n",
    "\n",
    "    # Plot Loss if data is available\n",
    "    if 'train_loss' in results and 'val_loss' in results:\n",
    "        train_loss = results['train_loss']\n",
    "        val_loss = results['val_loss']\n",
    "        min_loss = min(train_loss + val_loss)\n",
    "        max_loss = max(train_loss + val_loss)\n",
    "        axes[current_plot].plot(train_loss, label='Training Loss', color='blue')\n",
    "        axes[current_plot].plot(val_loss, label='Validation Loss', color='red')\n",
    "        axes[current_plot].set_title('Loss Over Epochs')\n",
    "        axes[current_plot].set_xlabel('Epoch')\n",
    "        axes[current_plot].set_ylabel('Loss')\n",
    "        axes[current_plot].set_ylim([min_loss - 0.05 * (max_loss - min_loss), max_loss + 0.05 * (max_loss - min_loss)])\n",
    "        axes[current_plot].legend()\n",
    "        current_plot += 1\n",
    "\n",
    "    # Plot PSNR if data is available\n",
    "    if 'train_acc' in results and 'val_acc' in results:\n",
    "        train_acc = results['train_acc']\n",
    "        val_acc = results['val_acc']\n",
    "        min_acc = min(train_acc + val_acc)\n",
    "        max_acc = max(train_acc + val_acc)\n",
    "        axes[current_plot].plot(train_acc, label='Training Acc', color='blue')\n",
    "        axes[current_plot].plot(val_acc, label='Validation Acc', color='red')\n",
    "        axes[current_plot].set_title('Acc Over Epochs')\n",
    "        axes[current_plot].set_xlabel('Epoch')\n",
    "        axes[current_plot].set_ylabel('Acc')\n",
    "        axes[current_plot].set_ylim([min_acc - 0.05 * (max_acc - min_acc), max_acc + 0.05 * (max_acc - min_acc)])\n",
    "        axes[current_plot].legend()\n",
    "        current_plot += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Image Quality Metrics\n",
    "\n",
    "1. Mean Squared Error(MSE) :  MSE loss function is evaluated only by the difference between the central pixels of Xi and the network output.\n",
    "\n",
    "$$\n",
    "L(\\Theta) = \\frac{1}{n} \\sum_{i=1}^n \\left \\| F(Y_i; \\Theta) - X_i \\right \\|^2,\n",
    "$$\n",
    "\n",
    "where n is the number of training examples. Using MSE as the loss functiion favors a high PSNR.\n",
    "\n",
    "Alternative evaluation metrics,\n",
    "\n",
    "1. Peak Signal To Noise Ratio(PSNR)\n",
    "\n",
    "2. Structural Similarity Index(SSIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(original : np.ndarray, target : np.ndarray) -> float:\n",
    "\n",
    "  \"\"\"\n",
    "  Compute the Mean Squared Error (MSE) between two images.\n",
    "\n",
    "  Parameters:\n",
    "  original (np.ndarray): The original image, expected to be a numpy array.\n",
    "  compressed (np.ndarray): The compressed or modified image, expected to be a numpy array.\n",
    "\n",
    "  Returns:\n",
    "  float: The MSE value.\n",
    "  \"\"\"\n",
    "\n",
    "  original_data = original.astype(np.float64)\n",
    "  target_data = target.astype(np.float64)\n",
    "\n",
    "  mse = np.mean((original_data - target_data)**2)\n",
    "  return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Peak Signal To Noise Ratio(PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def psnr(original : np.ndarray, target : np.ndarray) -> float:\n",
    "  \"\"\"\n",
    "  Compute the Peak Signal to Noise Ratio (PSNR) between two images.\n",
    "\n",
    "  Parameters:\n",
    "  original (numpy.ndarray): The original image.\n",
    "  compressed (numpy.ndarray): The compressed or modified image.\n",
    "\n",
    "  Returns:\n",
    "  float: The PSNR value in decibels (dB).\n",
    "  \"\"\"\n",
    "\n",
    "  original_data = original.astype(np.float64)\n",
    "  target_data = target.astype(np.float64)\n",
    "\n",
    "  mse = np.mean((original_data - target_data)**2)\n",
    "  if mse == 0:\n",
    "    # MSE is zero means no noise is present in the signal.\n",
    "    # Therefore, PSNR is infinite.\n",
    "    return float('inf')\n",
    "\n",
    "  max = 255.0\n",
    "  psnr_val = 20*np.log10(max / np.sqrt(mse))\n",
    "  return psnr_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Structural Similarity Index(SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim_lib\n",
    "from skimage.color import rgb2gray\n",
    "import numpy as np\n",
    "\n",
    "def ssim(original : np.ndarray, target : np.ndarray) -> float:\n",
    "  \"\"\"\n",
    "  Compute the Structural Similarity Index (SSIM) between two images.\n",
    "\n",
    "  Parameters:\n",
    "  original (np.ndarray): The original image.\n",
    "  compressed (np.ndarray): The compressed or modified image.\n",
    "\n",
    "  Returns:\n",
    "  float: The SSIM value.\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert images to grayscale if they are in color because SSIM is often computed in grayscale\n",
    "  if original.ndim == 3:\n",
    "    original = rgb2gray(original)\n",
    "  if target.ndim == 3:\n",
    "    target = rgb2gray(target)\n",
    "\n",
    "  # Dynamically determine the data range for SSIM calculation\n",
    "  # If the images are floating point, assume they are in [0, 1]\n",
    "  if original.dtype == np.float32 or original.dtype == np.float64:\n",
    "      data_range = 1\n",
    "  else:\n",
    "      # For integer types, use the maximum possible value of the dtype\n",
    "      data_range = np.iinfo(original.dtype).max\n",
    "  \n",
    "  ssim_value, _ = ssim_lib(original, target, data_range=data_range, full=True)\n",
    "  return ssim_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Combined Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "def combined_metric(original : np.ndarray, target : np.ndarray) -> Tuple:\n",
    "  \"\"\"\n",
    "  Combined metric using MSE, PSNR and SSIM\n",
    "\n",
    "  Parameters:\n",
    "  original (np.ndarray): The original image.\n",
    "  compressed (np.ndarray): The compressed or modified image.\n",
    "\n",
    "  Return:\n",
    "  Tuple: (MSE, PSNR, SSIM)\n",
    "  \"\"\"\n",
    "\n",
    "  mse_val = mse(original, target)\n",
    "  psnr_val = psnr(original, target)\n",
    "  ssim_val = ssim(original, target)\n",
    "\n",
    "  return (mse_val, psnr_val, ssim_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Transforming Data\n",
    "\n",
    "To prepare our dataset for training a Super-Resolution CNN (SRCNN), we need to simulate low-resolution (LR) images from our high-resolution (HR) images. The following steps are taken for each image in our HR dataset:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 LR Transformations\n",
    "\n",
    "1. **Apply Gaussian Blur**:\n",
    "   A Gaussian blur is applied to the HR images. This step is performed to smooth out the images and simulate the loss of detail that occurs in a real-world low-resolution image.\n",
    "\n",
    "2. **Downscale**:\n",
    "   The blurred HR images are then downsampled by a specified upscale factor.\n",
    "\n",
    "3. **Upscale**:\n",
    "   The downsampled images are then upscaled back to the original dimensions using bicubic interpolation.\n",
    "\n",
    "4. **Save Processed Images**:\n",
    "  The processed LR images are saved back to the filesystem for use in training the SRCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Gaussian Blur\n",
    "kernel_size = (9, 9)\n",
    "sigma = 1.5\n",
    "scale = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = input(\"Enter the operation that needs to performed on HR Image: \")\n",
    "\n",
    "# Print message about the processing type\n",
    "if operation == 'blur':\n",
    "    print(f\"Applying Gaussian Blur with kernel size {kernel_size} and sigma {sigma}.\")\n",
    "elif operation == 'downup':\n",
    "    print(\"Applying Down-and-Up sampling.\")\n",
    "elif operation == 'both':\n",
    "    print(f\"Applying both Gaussian Blur with kernel size {kernel_size} and sigma {sigma}, and Down-and-Up sampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def generate_lr_image(image: np.ndarray, operation: str, scale: int, kernel_size: Tuple[int, int] = (5, 5), sigma: float = 0) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  Processes the image based on the specified operation: Gaussian Blur, Down-and-Up sampling, or both,\n",
    "  for both grayscale and color images.\n",
    "\n",
    "  Parameters:\n",
    "  - image (np.ndarray): The input image to process. Can be grayscale or color.\n",
    "  - operation (str): The operation to perform - 'blur', 'downup', 'both'.\n",
    "  - scale (int): The downscaling factor for down-and-up sampling.\n",
    "  - kernel_size (Tuple[int, int]): The kernel size for Gaussian blur.\n",
    "  - sigma (float): The sigma value for Gaussian blur.\n",
    "\n",
    "  Returns:\n",
    "  - np.ndarray: Low-resolution image.\n",
    "  \"\"\"\n",
    "  # Check if the image is grayscale or color\n",
    "  if len(image.shape) == 2:\n",
    "    # Grayscale image\n",
    "    h, w = image.shape\n",
    "  else:\n",
    "    # Color image\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "  lr_image = image\n",
    "\n",
    "  if operation in ['blur', 'both']:\n",
    "    # Apply Gaussian Blur\n",
    "    lr_image = cv.GaussianBlur(lr_image, kernel_size, sigma)\n",
    "\n",
    "  if operation in ['downup', 'both']:\n",
    "    # Downscale and then upscale\n",
    "    if len(image.shape) == 2:\n",
    "      # Grayscale\n",
    "      lr_image = cv.resize(cv.resize(lr_image, (w // scale, h // scale), interpolation=cv.INTER_CUBIC), (w, h), interpolation=cv.INTER_CUBIC)\n",
    "    else:\n",
    "      # Color\n",
    "      lr_image = cv.resize(cv.resize(lr_image, (w // scale, h // scale), interpolation=cv.INTER_CUBIC), (w, h), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "  return lr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modcrop(img, modulo):\n",
    "\n",
    "  \"\"\"\n",
    "  Crop the image to a size that is divisible by the modulo value.\n",
    "\n",
    "  Parameters:\n",
    "  - img (np.ndarray): The input image to crop.\n",
    "  - modulo (int): The value to crop the image size to.\n",
    "\n",
    "  Returns:\n",
    "  - np.ndarray: The cropped image.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check the type of the input image(numpy ndarray or Image object)\n",
    "  if isinstance(img, PIL.Image.Image):\n",
    "    img = np.array(img)\n",
    "\n",
    "  if img.ndim == 2:\n",
    "    sz = img.shape\n",
    "    sz = sz - np.mod(sz, modulo)\n",
    "    img_cropped = img[0:sz[0], 0:sz[1]]\n",
    "  elif img.ndim == 3:\n",
    "    sz = img.shape[0:2]\n",
    "    sz = sz - np.mod(sz, modulo)\n",
    "    img_cropped = img[0:sz[0], 0:sz[1], :]\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported image dimensions\")\n",
    "\n",
    "  return img_cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Unpaired Dataset\n",
    "\n",
    "Note: We are working with only one scale(3x)\n",
    "* Classification:\n",
    "    * Training:     DIV2K(Train)\n",
    "    * Validation:   DIV2K(Validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIV2K Dataset\n",
    "div2k_train_path = \"/hard-disk-2/users/mpershad/DIV2K_train_HR\"\n",
    "div2k_validation_path = \"/hard-disk-2/users/mpershad/DIV2K_valid_HR\"\n",
    "\n",
    "patch_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/hard-disk-2/users/mpershad/SRCNN_DIV2K/Data/Unpaired/Classification/Train\"\n",
    "validation_path = \"/hard-disk-2/users/mpershad/SRCNN_DIV2K/Data/Unpaired/Classification/Validation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for generate_lr_image\n",
    "lr_image_args = {\n",
    "    'operation': operation,\n",
    "    'scale': scale,\n",
    "    'kernel_size': kernel_size,\n",
    "    'sigma': sigma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to random crops of images\n",
    "\n",
    "def _get_patch(*args, patch_size=224):\n",
    "    \"\"\"\n",
    "    Get a random patch of the specified size from the provided images.\n",
    "    \"\"\"\n",
    "    # Get the height and width of the image\n",
    "    h, w = args[0].shape[:2]\n",
    "\n",
    "    # Randomly select the top left corner of the patch\n",
    "    ix = np.random.randint(0, w - patch_size)\n",
    "    iy = np.random.randint(0, h - patch_size)\n",
    "\n",
    "    # Return the cropped images\n",
    "    return [img[iy: iy + patch_size, ix: ix + patch_size] for img in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def augment(*args, hflip=True, rot=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Augment the images by flipping and rotating them.\n",
    "    \"\"\"\n",
    "\n",
    "    hflip = hflip and random.random() < 0.5\n",
    "    vflip = rot and random.random() < 0.5\n",
    "    rot90 = rot and random.random() < 0.5\n",
    "\n",
    "    def _augment(img):\n",
    "        if hflip: img = img[:, ::-1, :]\n",
    "        if vflip: img = img[::-1, :, :]\n",
    "        if rot90: img = img.transpose(1, 0, 2)\n",
    "        return img\n",
    "    return [_augment(img) for img in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "# Function to generate and save the 224x224 patches for LR and HR image classification\n",
    "\n",
    "def get_patch(dataset_path: str, save_lr_path: str, save_hr_path: str, patch_size: int, num_patches: int, ext: str = \"*.png\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate and save the 224x224 patches for LR and HR image classification.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path (str): The path to the HR dataset.\n",
    "    - save_lr_path (str): The path to save the LR patches.\n",
    "    - save_hr_path (str): The path to save the HR patches.\n",
    "    - patch_size (int): The size of the patches to generate.\n",
    "    - num_patches (int): The number of patches to generate.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the directories to save the patches\n",
    "    if not os.path.exists(save_hr_path):\n",
    "        os.makedirs(save_hr_path)\n",
    "    if not os.path.exists(save_lr_path):\n",
    "        os.makedirs(save_lr_path)\n",
    "\n",
    "    # Get the list of image\n",
    "    filenames_path_list = sorted(Path(dataset_path).glob(f\"{ext}\"))\n",
    "\n",
    "    # Loop through the images\n",
    "    for filename_path in tqdm(filenames_path_list):\n",
    "        img = Image.open(filename_path).convert('RGB')\n",
    "\n",
    "        # Use the modcrop function to crop the image\n",
    "        img = modcrop(img, 3)\n",
    "\n",
    "        # Use the _get_patch function to get the patches\n",
    "        for i in range(num_patches):\n",
    "            hr_patch, lr_patch = _get_patch(np.array(img), generate_lr_image(np.array(img), **lr_image_args), patch_size=patch_size)\n",
    "\n",
    "            # Augment the patches\n",
    "            hr_patch, lr_patch = augment(hr_patch, lr_patch)\n",
    "\n",
    "            # Save the serialized patches\n",
    "            hr_patch_path = os.path.join(save_hr_path, f\"{filename_path.stem}_{i+1}.png\")\n",
    "            lr_patch_path = os.path.join(save_lr_path, f\"{filename_path.stem}_{i+1}.png\")\n",
    "\n",
    "            Image.fromarray(hr_patch).save(hr_patch_path)\n",
    "            Image.fromarray(lr_patch).save(lr_patch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the get_patch to generate training data\n",
    "# get_patch(dataset_path=div2k_train_path,\n",
    "#           save_lr_path=os.path.join(train_path, \"LR\"),\n",
    "#           save_hr_path=os.path.join(train_path, \"HR\"),\n",
    "#           patch_size=patch_size,\n",
    "#           num_patches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the get_patch function to generate validation data\n",
    "# get_patch(dataset_path=div2k_validation_path,\n",
    "#           save_lr_path=os.path.join(validation_path, \"LR\"),\n",
    "#           save_hr_path=os.path.join(validation_path, \"HR\"),\n",
    "#           patch_size=patch_size,\n",
    "#           num_patches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_dir(path : str) -> None:\n",
    "  \"\"\"\n",
    "  Walk through Directory returning its content\n",
    "  \"\"\"\n",
    "\n",
    "  for dirpath, dirname, filenames in os.walk(path):\n",
    "    print(f\"There are {len(filenames)} images in the {dirpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/hard-disk-2/users/mpershad/SRCNN_DIV2K/Data/Unpaired/Classification'\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Loading Images data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Loading Images using ImageFolder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Tansform for preforming same preprocessing as VGG19 before extracting features\n",
    "vgg_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_transform = vgg_transform\n",
    "\n",
    "validation_transform = vgg_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Use ImageFolder to create dataset(s)\n",
    "from torchvision import datasets\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_path,\n",
    "                                     transform=train_transform,\n",
    "                                     target_transform=None) # Transform for the target (our data is in standard classification format, labels are inferred form the directory name)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(root=validation_path,\n",
    "                                        transform=validation_transform,\n",
    "                                        target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class name as list\n",
    "class_names = train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names as dict\n",
    "class_names_dict = train_dataset.class_to_idx\n",
    "class_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the dataset\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index on the train_data to get a single Image and label\n",
    "img, label = train_dataset[0]\n",
    "print(f\"Image Shape: {img.shape}\")\n",
    "print(f\"Image DataType: {img.dtype}\")\n",
    "print(f\"Image Label: {class_names[label]}\")\n",
    "print(f\"Label dataType: {type(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Plot a random Image and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a radom image\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_dataset), size=[1]).item()\n",
    "img, label = train_dataset[random_idx]\n",
    "img = img.permute(1, 2, 0)\n",
    "print(f\"Image Shape: {img.shape}\")\n",
    "print(f\"Image DataType: {img.dtype}\")\n",
    "print(f\"Image Label: {class_names[label]}\")\n",
    "\n",
    "# plot the image\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Turn custom Dataset into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn the dataset into iterable(batches)\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=os.cpu_count(),\n",
    "                              shuffle=True)\n",
    "\n",
    "validation_dataloader = DataLoader(dataset=validation_dataset,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     num_workers=os.cpu_count(),\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of Training Data: {len(train_dataloader)}\")\n",
    "print(f\"Length of Training Data: {len(validation_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Tracking experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={ 'start_epoch': 0,\n",
    "         'end_epoch': 200, \n",
    "         'architecture': 'VGGClassifier', \n",
    "         'checkpoint_interval': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Try to Log in to your W&B account\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"Successfully logged in to W&B!\")\n",
    "\n",
    "    # Initialize a W&B run\n",
    "    wandb.init(project='SRCNN+VGG', entity='pershadmayank', config=config)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during login:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Setup Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Initialize the VGG19 model\n",
    "classifier = VGG19Classifier(num_features=28*28*512)\n",
    "\n",
    "# Setup the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = str(input(\"Enter model name:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the wandb.config values\n",
    "model_results = train(model=classifier,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      test_dataloader=validation_dataloader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=loss_fn,\n",
    "                      path=model_name,\n",
    "                      start_epoch=config['start_epoch'],\n",
    "                      end_epoch=config['end_epoch'],\n",
    "                      device=device,\n",
    "                      checkpoint_interval=config['checkpoint_interval'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Save Results Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "metrics_path =f'/hard-disk-2/users/mpershad/SRCNN_DIV2K/Metrics/Classifier'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "  os.makedirs(metrics_path)\n",
    "\n",
    "pickle_file_path = os.path.join(metrics_path, f\"metrics_{model_name}.pkl\")\n",
    "\n",
    "\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(model_results, file)\n",
    "\n",
    "print(f\"Results saved to {pickle_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
